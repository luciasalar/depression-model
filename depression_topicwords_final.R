#segment words
#install.packages("rmmseg4j", repos="http://R-Forge.R-project.org",type='source')
require(tm)
require(ggplot2)
library(rmmseg4j)
library(wordcloud)
library(proxy)

#this script generate a 

#we used a separate set of data to generate topics, 

w <- read.csv("weibo_scrapy.csv", header = F, fill=TRUE,row.names=NULL)
colnames(w) <- c("name", "gender", "location", "intro", "weibo_num","follow","fans","like","retweet","comment","time","phone","weibo")
write.csv(w,file='w.csv')

#clean all the non original data
w2 <- w[- grep("æ˜ å®¢iOS|å¾®åšç­‰çº§|ç½‘æ˜“äº‘éŸ³ä¹|è”æžFM|æ˜Žæ˜ŸåŠ¿åŠ›æ¦œ|QQéŸ³ä¹|Feelapp|æ‰‹æœºNBAä¸­æ–‡å®˜ç½‘|å¾®åšçº¢åŒ…|ç²‰ä¸çº¢åŒ…|å¾®å¡åˆ¸|ä¸€ç›´æ’­|LOFTER|å¾®åšä¼šå‘˜ä¸­å¿ƒ|å¾®åšæ´»åŠ¨|ç™¾è¯æ–©å›¾èƒŒå•è¯|è®©çº¢åŒ…é£žæ‰‹æœºç‰ˆ|å¤©ä¸‹3|å¾®åšç­‰çº§|åˆ†äº«æŒ‰é’®|èƒŒå•è¯", w$phone),]
w3 <- w2[- grep("å›´è§‚ä¸–ç•Œæ¯é¢†çŽ°é‡‘|å›´è§‚ä¸–ç•Œæ¯|å¾®åšè¸¢çƒ|å¾®è¯é¢˜|ä½ ç”»æˆ‘çŒœæ¬¢ä¹ç‰ˆ|å¤©ä¸‹3|åˆ†äº«æŒ‰é’®|å¾®ç›˜|æŠ•ç¥¨|å¾®åšç”µè§†é›·è¾¾|ç–¯ç‹‚å¼€å®ç®±|çŸ¥å‘½|å…¨æ°‘Kæ­Œ|å”±å§|å¾®åšä¹‹å¤œ|å‹‹ç« é¦†|ä¸“é¢˜|å¾®ä¸“é¢˜|æµ‹è¯•|å¤šæŽ¨|æ–°æµªåšå®¢|å•ªå•ª|æ¨±æ¡ƒå°ä¸¸å­|å¤šæŽ¨|Nikepluschina|è…¾è®¯æ–°é—»å®¢æˆ·ç«¯|æ»´æ»´å‡ºè¡Œ", w2$phone),]
#remove public account
w3 <- w3[- grep("æ²»ç–—|ç§‘æ™®|åŒ»é™¢", w3$name),]
w3 <- w3[- grep("æ»´æ»´å‡ºè¡Œ|ðŸ›«", w3$weibo),]

#remove LOCATION, BRAKETS, PUNCUTATION, NUMBERS AND ENGLISH 
w3$weibo <- gsub("\\S+Â·\\S+", "", w3$weibo)
w3$weibo <- gsub("\\S+ æˆ‘åœ¨:\\S+", "", w3$weibo)
w3$weibo <- gsub("\\S+ æˆ‘åœ¨è¿™é‡Œ:\\S+", "", w3$weibo)
#w3$weibo <- gsub("\\S+ æˆ‘åœ¨.?:\\S+", "", w3$weibo)
w3$weibo <- gsub("\\s*\\[[^\\)]+\\]","", w3$weibo)
w3$weibo <- gsub("\\s*\\ã€Š[^\\)]+\\ã€‹","", w3$weibo)
w3$weibo <- gsub("[[:punct:]]", "", w3$weibo)
w3$weibo <- gsub("[[:digit:]]+", "", w3$weibo)
w3$weibo <- gsub("åˆ†äº«å›¾ç‰‡", "", w3$weibo)
w3$weibo <- gsub("([A-Za-z]+).*", "", w3$weibo)

#remove empty rows

#order a column
# dd[ order(dd[,4], dd[,1]), ]
# dput(w3[1:10,13])

w4 <- w3[-which(w3$weibo == "" | w3$weibo == " " | w3$weibo == "  "), ]

#remove stopwords
w4$weibo <- gsub("çš„|äº†|åœ¨|æ˜¯|æˆ‘|æœ‰|å’Œ|å°±|äºº|éƒ½|ä¸€ä¸ª|ä¸Š|ä¹Ÿ|å¾ˆ|ä½ |ä¼š|ç€|çœ‹|è‡ªå·±|æˆ‘ä»¬|å•Š|å•¦", "", w4$weibo)

w5 <- w4

ss <- as.character(w5$weibo)

seg <- mmseg4j(ss)

#NGramTokenizer(' ä¸­åŽäººæ°‘å…±å’Œå›½æˆç«‹äºŽ1949 å¹´')

#corpus
corpus.dep <- Corpus(VectorSource(seg))
tdm.dep <- TermDocumentMatrix(corpus.dep, control = list(weighting = weightTfIdf))

m1 <- as.matrix(tdm.dep)
v <- sort(rowSums(m1), decreasing = TRUE)
d <- data.frame(word = names(v), freq = v)
par(family='STKaiti')
cloud_1 <- wordcloud(d$word, d$freq, min.freq = 50, random.order = F, ordered.colors = F, 
                     colors = rainbow(length(row.names(m1))))

write.csv(d, "depression_topic.csv")

###we rated positive, negative and health related words in depression_topic.csv, see dep_word.xls in data folder







